{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inside Unsupervised Learning: Generative Models and Recommender Systems\n",
    "## Explore generative models and build movie recommender systems\n",
    "### by Ankur A. Patel + O'Reilly Media, Inc.\n",
    "\n",
    "## Overview\n",
    "In this notebook, we will explore:\n",
    "#1 The difference between discriminative and generative models\n",
    "#2 Why generative models are so powerful\n",
    "#3 The various types of generative models used today\n",
    "#4 How to train a simple generative model (restricted Boltzmann machines)\n",
    "\n",
    "Specifically, we will train RBMs to build a movie recommender system. After this course, you should be familiar with generative models and know a bit about how to train recommender systems in your field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Load in the movie ratings dataset and prepare it.\n",
    "\n",
    "We will use the MovieLens 20M Dataset. The original ratings file has 20,000,263 ratings, 27,278 movies, and 138,493 users.\n",
    "\n",
    "We will use a reduced version of this data for this course. Our reduced version has data for the top 1,000 movies and a sample of 1,000 users.\n",
    "\n",
    "Since not all 1,000 users rated all 1,000 movies, there are 664,550 ratings in total.\n",
    "\n",
    "For each movie review, we have the original movieID, the original userID, the movie rating the user assigned, and the timestamp. We have two additional fields, the new movieID and the new userID, which are just reindexed versions of the original movieID and the original userID for the reduced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, re\n",
    "import pickle, gzip, datetime\n",
    "from datetime import datetime\n",
    "\n",
    "'''Data Viz'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n",
    "\n",
    "'''Algos'''\n",
    "import lightgbm as lgb\n",
    "\n",
    "'''TensorFlow and Keras'''\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.layers import BatchNormalization, Input, Lambda\n",
    "from keras.layers import Embedding, Flatten, dot\n",
    "from keras import regularizers\n",
    "from keras.losses import mse, binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "os.chdir('/home/jovyan/')\n",
    "current_path = os.getcwd()\n",
    "pickle_file = '/data/movielens_data/ratingReducedPickle'\n",
    "ratingDF = pd.read_pickle(current_path + pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View 5 rows of the data\n",
    "ratingDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics on reduced dataset\n",
    "n_users = ratingDF.userId.unique().shape[0]\n",
    "n_movies = ratingDF.movieId.unique().shape[0]\n",
    "n_ratings = len(ratingDF)\n",
    "avg_ratings_per_user = n_ratings/n_users\n",
    "\n",
    "print('Number of unique users: ', n_users)\n",
    "print('Number of unique movies: ', n_movies)\n",
    "print('Number of total ratings: ', n_ratings)\n",
    "print('Average number of ratings per user: ', avg_ratings_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into validation and test, such that each is 5% of the dataset\n",
    "X_train, X_test = train_test_split(ratingDF, test_size=0.10, \\\n",
    "                                   shuffle=True, random_state=2018)\n",
    "\n",
    "X_validation, X_test = train_test_split(X_test, test_size=0.50, \\\n",
    "                                        shuffle=True, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm size of train, validation, and test datasets\n",
    "print('Size of train set: ', len(X_train))\n",
    "print('Size of validation set: ', len(X_validation))\n",
    "print('Size of test set: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ratings matrix for train\n",
    "ratings_train = np.zeros((n_users, n_movies))\n",
    "for row in X_train.itertuples():\n",
    "    ratings_train[row[6]-1, row[5]-1] = row[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sparsity of the train ratings matrix\n",
    "sparsity = float(len(ratings_train.nonzero()[0]))\n",
    "sparsity /= (ratings_train.shape[0] * ratings_train.shape[1])\n",
    "sparsity *= 100\n",
    "print('Sparsity: {:4.2f}%'.format(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ratings matrix for validation\n",
    "ratings_validation = np.zeros((n_users, n_movies))\n",
    "for row in X_validation.itertuples():\n",
    "    ratings_validation[row[6]-1, row[5]-1] = row[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ratings matrix for test\n",
    "ratings_test = np.zeros((n_users, n_movies))\n",
    "for row in X_test.itertuples():\n",
    "    ratings_test[row[6]-1, row[5]-1] = row[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sparsity of the validation ratings matrix\n",
    "sparsity = float(len(ratings_validation.nonzero()[0]))\n",
    "sparsity /= (ratings_validation.shape[0] * ratings_validation.shape[1])\n",
    "sparsity *= 100\n",
    "print('Sparsity: {:4.2f}%'.format(sparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #1 - Naive Recommender System A\n",
    "Assign naive 3.5 rating to each missing movie review and calculate the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store actual ratings for the validation set\n",
    "actual_validation = ratings_validation[ratings_validation.nonzero()].flatten()\n",
    "\n",
    "# Assign \"naive\" 3.5 rating to missing values in the validation set\n",
    "pred_validation = np.zeros((len(X_validation),1))\n",
    "pred_validation[pred_validation==0] = 3.5\n",
    "\n",
    "# Calculate mean square error using this naive prediction\n",
    "naive_prediction = mean_squared_error(pred_validation, actual_validation)\n",
    "print('Mean squared error using naive prediction:', naive_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #2 - Naive Recommender System B\n",
    "Predict a user's rating based on user's average rating for all other movies the user has rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average user rating recommender system\n",
    "ratings_validation_prediction = np.zeros((n_users, n_movies))\n",
    "i = 0\n",
    "for row in ratings_train:\n",
    "    ratings_validation_prediction[i][ratings_validation_prediction[i]==0] \\\n",
    "        = np.mean(row[row>0])\n",
    "    i += 1\n",
    "\n",
    "pred_validation = ratings_validation_prediction \\\n",
    "    [ratings_validation.nonzero()].flatten()\n",
    "user_average = mean_squared_error(pred_validation, actual_validation)\n",
    "print('Mean squared error using user average:', user_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #3 - Naive Recommender System C\n",
    "Predict a user's rating for a movie based on the average rating other users have given that movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average movie rating recommender system\n",
    "ratings_validation_prediction = np.zeros((n_users, n_movies)).T\n",
    "i = 0\n",
    "for row in ratings_train.T:\n",
    "    ratings_validation_prediction[i][ratings_validation_prediction[i]==0] \\\n",
    "        = np.mean(row[row>0])\n",
    "    i += 1\n",
    "\n",
    "ratings_validation_prediction = ratings_validation_prediction.T\n",
    "pred_validation = ratings_validation_prediction \\\n",
    "    [ratings_validation.nonzero()].flatten()\n",
    "movie_average = mean_squared_error(pred_validation.astype(np.float64), \\\n",
    "                                   actual_validation.astype(np.float64))\n",
    "print('Mean squared error using movie average:', movie_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #4 - Recommender System using Matrix Factorization\n",
    "We will use 1 latent factor, but you could experiment with a different number of latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latent_factors = 1\n",
    "\n",
    "user_input = Input(shape=[1], name='user')\n",
    "user_embedding = Embedding(input_dim=n_users + 1, \\\n",
    "                           output_dim=n_latent_factors, \\\n",
    "                           name='user_embedding')(user_input)\n",
    "user_vec = Flatten(name='flatten_users')(user_embedding)\n",
    "\n",
    "movie_input = Input(shape=[1], name='movie')\n",
    "movie_embedding = Embedding(input_dim=n_movies + 1, \\\n",
    "                            output_dim=n_latent_factors,\n",
    "                            name='movie_embedding')(movie_input)\n",
    "movie_vec = Flatten(name='flatten_movies')(movie_embedding)\n",
    "\n",
    "product = dot([movie_vec, user_vec], axes=1)\n",
    "model = Model(inputs=[user_input, movie_input], outputs=product)\n",
    "model.compile('adam', 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=[X_train.newUserId, X_train.newMovieId], \\\n",
    "                    y=X_train.rating, epochs=10, \\\n",
    "                    validation_data=([X_validation.newUserId, \\\n",
    "                    X_validation.newMovieId], X_validation.rating), \\\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(history.history['val_loss'][:]).plot(logy=False)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Error\")\n",
    "print('Minimum MSE: ', min(history.history['val_loss']))\n",
    "\n",
    "preds = model.predict(x=[X_validation.newUserId, X_validation.newMovieId])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE on train\n",
    "preds = model.predict(x=[X_train.newUserId, X_train.newMovieId])\n",
    "mean_squared_error(preds, X_train.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE on validation\n",
    "preds = model.predict(x=[X_validation.newUserId, X_validation.newMovieId])\n",
    "mean_squared_error(preds, X_validation.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE on test\n",
    "preds = model.predict(x=[X_test.newUserId, X_test.newMovieId])\n",
    "mean_squared_error(preds, X_test.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #5 - Recommender System using RBMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RBM class\n",
    "class RBM(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, \n",
    "                 learning_rate, epochs, batchsize):\n",
    "        # Define hyperparameters\n",
    "        self._input_size = input_size\n",
    "        self._output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        \n",
    "        # Initialize weights and biases using zero matrices\n",
    "        self.w = np.zeros([input_size, output_size], dtype=np.float32)\n",
    "        self.hb = np.zeros([output_size], dtype=np.float32)\n",
    "        self.vb = np.zeros([input_size], dtype=np.float32)\n",
    "\n",
    "    def prob_h_given_v(self, visible, w, hb):\n",
    "        return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n",
    "\n",
    "    def prob_v_given_h(self, hidden, w, vb):\n",
    "        return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n",
    "    \n",
    "    def sample_prob(self, probs):\n",
    "        return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))\n",
    "\n",
    "    def train(self, X):\n",
    "        _w = tf.placeholder(tf.float32, [self._input_size, self._output_size])\n",
    "        _hb = tf.placeholder(tf.float32, [self._output_size])\n",
    "        _vb = tf.placeholder(tf.float32, [self._input_size])\n",
    "        \n",
    "        prv_w = np.zeros([self._input_size, self._output_size], dtype=np.float32)\n",
    "        prv_hb = np.zeros([self._output_size], dtype=np.float32)\n",
    "        prv_vb = np.zeros([self._input_size], dtype=np.float32)\n",
    "        \n",
    "        cur_w = np.zeros([self._input_size, self._output_size], dtype=np.float32)\n",
    "        cur_hb = np.zeros([self._output_size], dtype=np.float32)\n",
    "        cur_vb = np.zeros([self._input_size], dtype=np.float32)\n",
    "        \n",
    "        v0 = tf.placeholder(tf.float32, [None, self._input_size])\n",
    "        h0 = self.sample_prob(self.prob_h_given_v(v0, _w, _hb))\n",
    "        v1 = self.sample_prob(self.prob_v_given_h(h0, _w, _vb))\n",
    "        h1 = self.prob_h_given_v(v1, _w, _hb)\n",
    "        \n",
    "        positive_grad = tf.matmul(tf.transpose(v0), h0)\n",
    "        negative_grad = tf.matmul(tf.transpose(v1), h1)\n",
    "        \n",
    "        update_w = _w + self.learning_rate * \\\n",
    "            (positive_grad - negative_grad) / tf.to_float(tf.shape(v0)[0])\n",
    "        update_vb = _vb + self.learning_rate * tf.reduce_mean(v0 - v1, 0)\n",
    "        update_hb = _hb + self.learning_rate * tf.reduce_mean(h0 - h1, 0)\n",
    "        \n",
    "        err = tf.reduce_mean(tf.square(v0 - v1))\n",
    "        \n",
    "        error_list = []\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            for epoch in range(self.epochs):\n",
    "                for start, end in zip(range(0, len(X), \\\n",
    "                        self.batchsize),range(self.batchsize,len(X), \\\n",
    "                                              self.batchsize)):\n",
    "                    batch = X[start:end]\n",
    "                    cur_w = sess.run(update_w, feed_dict={v0: batch, \\\n",
    "                                    _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_hb = sess.run(update_hb, feed_dict={v0: batch, \\\n",
    "                                    _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_vb = sess.run(update_vb, feed_dict={v0: batch, \\\n",
    "                                    _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    prv_w = cur_w\n",
    "                    prv_hb = cur_hb\n",
    "                    prv_vb = cur_vb\n",
    "                error = sess.run(err, feed_dict={v0: X, \\\n",
    "                                _w: cur_w, _vb: cur_vb, _hb: cur_hb})\n",
    "                print ('Epoch: %d' % epoch,'reconstruction error: %f' % error)\n",
    "                error_list.append(error)\n",
    "            self.w = prv_w\n",
    "            self.hb = prv_hb\n",
    "            self.vb = prv_vb\n",
    "            return error_list\n",
    "\n",
    "    def rbm_output(self, X):\n",
    "        \n",
    "        input_X = tf.constant(X)\n",
    "        _w = tf.constant(self.w)\n",
    "        _hb = tf.constant(self.hb)\n",
    "        _vb = tf.constant(self.vb)\n",
    "        out = tf.nn.sigmoid(tf.matmul(input_X, _w) + _hb)\n",
    "        visibleGen = tf.nn.sigmoid(tf.matmul(out, tf.transpose(_w)) + _vb)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(out)\n",
    "            sess.run(visibleGen)\n",
    "            return sess.run(out), sess.run(visibleGen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin the training cycle\n",
    "\n",
    "# Convert inputX into float32\n",
    "inputX = ratings_train/5.0\n",
    "inputX = inputX.astype(np.float32)\n",
    "\n",
    "# Define the parameters of the RBMs we will train\n",
    "rbm=RBM(1000,10,0.1,300,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train RBM model\n",
    "err = rbm.train(inputX)\n",
    "outputX, reconstructedX = rbm.rbm_output(inputX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reconstruction errors\n",
    "pd.Series(err).plot(logy=False)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reconstruction Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find scaling factor based on training set\n",
    "actual_train = ratings_train[ratings_train.nonzero()].flatten()\n",
    "train_mean = np.mean(actual_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE on train set\n",
    "predictionsArray = reconstructedX\n",
    "pred_train = \\\n",
    "    predictionsArray[ratings_train.nonzero()].flatten()\n",
    "pred_train = pred_train * train_mean/np.mean(pred_train)\n",
    "actual_train = \\\n",
    "    ratings_train[ratings_train.nonzero()].flatten()\n",
    "\n",
    "rbm_prediction = mean_squared_error(pred_train, actual_train)\n",
    "print('Mean squared error using RBM prediction:', rbm_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE on validation set\n",
    "predictionsArray = reconstructedX\n",
    "pred_validation = \\\n",
    "    predictionsArray[ratings_validation.nonzero()].flatten()\n",
    "pred_validation = pred_validation * train_mean/np.mean(pred_validation)\n",
    "actual_validation = \\\n",
    "    ratings_validation[ratings_validation.nonzero()].flatten()\n",
    "\n",
    "rbm_prediction = mean_squared_error(pred_validation, actual_validation)\n",
    "print('Mean squared error using RBM prediction:', rbm_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View generated predictions on validation set\n",
    "pred_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View actual ratings in validation set\n",
    "actual_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Train the RBM with an input layer of 1,000 nodes, a output layer of 50 nodes, a learning rate of 0.3, and a batch size of 32 for 100 epochs.\n",
    "\n",
    "Then, evaluate the performance on the train set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin the training cycle\n",
    "\n",
    "# Convert inputX into float32\n",
    "inputX = ratings_train/5.0\n",
    "inputX = inputX.astype(np.float32)\n",
    "\n",
    "# Define the parameters of the RBMs we will train\n",
    "rbm=RBM(#input nodes, #output nodes, #learning rate, #epochs, #batch size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RBM model\n",
    "err = rbm.train(#Fill in)\n",
    "outputX, reconstructedX = rbm.rbm_output(#Fill in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reconstruction errors\n",
    "pd.Series(err).plot(logy=False)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reconstruction Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE on train set\n",
    "predictionsArray = reconstructedX\n",
    "pred_train = \\\n",
    "    predictionsArray[ratings_train.nonzero()].flatten()\n",
    "pred_train = pred_train * train_mean/np.mean(pred_train)\n",
    "actual_train = \\\n",
    "    ratings_train[ratings_train.nonzero()].flatten()\n",
    "\n",
    "rbm_prediction = mean_squared_error(pred_train, actual_train)\n",
    "print('Mean squared error using RBM prediction:', rbm_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE on validation set\n",
    "predictionsArray = reconstructedX\n",
    "pred_validation = \\\n",
    "    predictionsArray[ratings_validation.nonzero()].flatten()\n",
    "pred_validation = pred_validation * train_mean/np.mean(pred_validation)\n",
    "actual_validation = \\\n",
    "    ratings_validation[ratings_validation.nonzero()].flatten()\n",
    "\n",
    "rbm_prediction = mean_squared_error(pred_validation, actual_validation)\n",
    "print('Mean squared error using RBM prediction:', rbm_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin the training cycle\n",
    "\n",
    "# Convert inputX into float32\n",
    "inputX = ratings_train/5.0\n",
    "inputX = inputX.astype(np.float32)\n",
    "\n",
    "# Define the parameters of the RBMs we will train\n",
    "rbm=RBM(1000, 50, 0.3, 100, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train RBM model\n",
    "err = rbm.train(inputX)\n",
    "outputX, reconstructedX = rbm.rbm_output(inputX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reconstruction errors\n",
    "pd.Series(err).plot(logy=False)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reconstruction Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE on train set\n",
    "predictionsArray = reconstructedX\n",
    "pred_train = \\\n",
    "    predictionsArray[ratings_train.nonzero()].flatten()\n",
    "pred_train = pred_train * train_mean/np.mean(pred_train)\n",
    "actual_train = \\\n",
    "    ratings_train[ratings_train.nonzero()].flatten()\n",
    "\n",
    "rbm_prediction = mean_squared_error(pred_train, actual_train)\n",
    "print('Mean squared error using RBM prediction:', rbm_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE on validation set\n",
    "predictionsArray = reconstructedX\n",
    "pred_validation = \\\n",
    "    predictionsArray[ratings_validation.nonzero()].flatten()\n",
    "pred_validation = pred_validation * train_mean/np.mean(pred_validation)\n",
    "actual_validation = \\\n",
    "    ratings_validation[ratings_validation.nonzero()].flatten()\n",
    "\n",
    "rbm_prediction = mean_squared_error(pred_validation, actual_validation)\n",
    "print('Mean squared error using RBM prediction:', rbm_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we explored generative models and RBMs.\n",
    "\n",
    "We then built non-generative movie recommender systems.\n",
    "\n",
    "And then we built a generative model-based movie recommender system using RBMs.\n",
    "\n",
    "The generative model-based movie recommender system did not fare as well as the non-generative movie recommender systems.\n",
    "\n",
    "But, there is a lot of room for improvement. This is what makes building great generative models and recommender systems difficult.\n",
    "\n",
    "Congratulations, you've finished this course! \n",
    "Go build more recommender systems in your field!\n",
    "\n",
    "The next course in the Inside Unsupervised Learning series is Anomaly Detection Using Unsupervised Learning.\n",
    "https://learning.oreilly.com/live-training/courses/inside-unsupervised-learning-anomaly-detection-using-dimensionality-reduction/0636920289654/\n",
    "\n",
    "You could also learn more about Unsupervised Learning in my book, Hands-on Unsupervised Learning Using Python.\n",
    "https://www.unsupervisedlearningbook.com/\n",
    "\n",
    "Finally, there is a Slack commmunity for you to join.\n",
    "https://join.slack.com/t/unsupervisedbook/shared_invite/enQtNjQzMjMyMjAyMjQyLTE3MWZlNTU0N2U3Zjg2MzNjMjM5MTI4YmIxY2ExOTIzNzA0ZTVmYjAyYWJhMjE1ZTc3NDRiNDIwOGM5YmY0M2E"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
