{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inside Unsupervised Learning: Feature Extraction using Autoencoders and Semi-Supervised Learning\n",
    "## Explore automatic feature engineering using autoencoders and build semi-supervised solutions\n",
    "### by Ankur A. Patel + O'Reilly Media, Inc.\n",
    "\n",
    "## Overview\n",
    "In this notebook, you will understand how to:\n",
    "#1. Learn representations using autoencoders\n",
    "#2. Develop a semi-supervised learning fraud detection solution\n",
    "\n",
    "Specifically, we will TensorFlow and Keras to build autoencoders and feed autoencoder representations (i.e., the hidden layer of the neural net) into a supervised model to build a semi-supervised credit card fraud detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Let's load in the credit card transactions dataset and prepare it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, re, pickle, gzip\n",
    "\n",
    "'''Data Viz'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "'''Algos'''\n",
    "import lightgbm as lgb\n",
    "\n",
    "'''TensorFlow and Keras'''\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.layers import BatchNormalization, Input, Lambda\n",
    "from keras import regularizers\n",
    "from keras.losses import mse, binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "data1 = pd.read_csv('../data/credit_card_data/credit_card_data_part1.csv')\n",
    "data2 = pd.read_csv('../data/credit_card_data/credit_card_data_part2.csv')\n",
    "data = data1.append(data2)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "dataX = data.copy().drop(['Class','Time','Unnamed: 0'],axis=1)\n",
    "dataY = data['Class'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale data\n",
    "featuresToScale = dataX.columns\n",
    "sX = pp.StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "dataX.loc[:,featuresToScale] = sX.fit_transform(dataX[featuresToScale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(dataX, dataY, test_size=0.33, \\\n",
    "                     random_state=2018, stratify=dataY)\n",
    "\n",
    "X_train_AE = X_train.copy()\n",
    "X_test_AE = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def anomalyScores(originalDF, reducedDF):\n",
    "    loss = np.sum((np.array(originalDF) - \\\n",
    "                   np.array(reducedDF))**2, axis=1)\n",
    "    loss = pd.Series(data=loss,index=originalDF.index)\n",
    "    loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define plotting function\n",
    "def plotResults(trueLabels, anomalyScores, returnPreds = False):\n",
    "    preds = pd.concat([trueLabels, anomalyScores], axis=1)\n",
    "    preds.columns = ['trueLabel', 'anomalyScore']\n",
    "    precision, recall, thresholds = \\\n",
    "        precision_recall_curve(preds['trueLabel'], \\\n",
    "                               preds['anomalyScore'])\n",
    "    average_precision = average_precision_score( \\\n",
    "                        preds['trueLabel'], preds['anomalyScore'])\n",
    "    \n",
    "    plt.step(recall, precision, color='k', alpha=0.7, where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    \n",
    "    plt.title('Precision-Recall curve: Average Precision = \\\n",
    "        {0:0.2f}'.format(average_precision))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(preds['trueLabel'], \\\n",
    "                                     preds['anomalyScore'])\n",
    "    areaUnderROC = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\n",
    "    plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic: Area under the \\\n",
    "        curve = {0:0.2f}'.format(areaUnderROC))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    if returnPreds==True:\n",
    "        return preds, average_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #1 - Complete Autoencoder\n",
    "Two layer complete autoencoder with linear activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call neural network API\n",
    "model = Sequential()\n",
    "\n",
    "# Apply linear activation function to input layer\n",
    "# Generate hidden layer with 29 nodes, the same as the input layer\n",
    "model.add(Dense(units=29, activation='linear',input_dim=29))\n",
    "\n",
    "# Apply linear activation function to hidden layer\n",
    "# Generate output layer with 29 nodes\n",
    "model.add(Dense(units=29, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(x=X_train_AE, y=X_train_AE,\n",
    "                    epochs=num_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_train_AE, X_train_AE),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "predictions = model.predict(X_test, verbose=1)\n",
    "anomalyScoresAE = anomalyScores(X_test, predictions)\n",
    "preds = plotResults(y_test, anomalyScoresAE, True)\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #2 - Undercomplete Autoencoder\n",
    "Two layer undercomplete autoencoder (27 nodes) with linear activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call neural network API\n",
    "model = Sequential()\n",
    "\n",
    "# Apply linear activation function to input layer\n",
    "# Generate hidden layer with 27 nodes\n",
    "model.add(Dense(units=27, activation='linear',input_dim=29))\n",
    "\n",
    "# Apply linear activation function to hidden layer\n",
    "# Generate output layer with 29 nodes\n",
    "model.add(Dense(units=29, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(x=X_train_AE, y=X_train_AE,\n",
    "                    epochs=num_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_train_AE, X_train_AE),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "predictions = model.predict(X_test, verbose=1)\n",
    "anomalyScoresAE = anomalyScores(X_test, predictions)\n",
    "preds, avgPrecision = plotResults(y_test, anomalyScoresAE, True)\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Build an ML Solution When You Lack Labels\n",
    "Let's prepare the credit card transactions dataset to simulate a partially labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 90% of the labels from the training set\n",
    "toDrop = y_train[y_train==1].sample(frac=0.90,random_state=2018)\n",
    "X_train.drop(labels=toDrop.index,inplace=True)\n",
    "y_train.drop(labels=toDrop.index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new function to assess precision at a given recall threshold\n",
    "def precisionAnalysis(df, column, threshold):\n",
    "    df.sort_values(by=column, ascending=False, inplace=True)\n",
    "    threshold_value = threshold*df.trueLabel.sum()\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < threshold_value+1:\n",
    "        if df.iloc[j][\"trueLabel\"]==1:\n",
    "            i += 1\n",
    "        j += 1\n",
    "    return df, i/j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for k-fold cross-validation\n",
    "k_fold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection using Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light GBM Solution \n",
    "params_lightGB = {\n",
    "    'task': 'train',\n",
    "    'application':'binary',\n",
    "    'num_class':1,\n",
    "    'boosting': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'metric_freq':50,\n",
    "    'is_training_metric':False,\n",
    "    'max_depth':4,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 1.0,\n",
    "    'bagging_fraction': 1.0,\n",
    "    'bagging_freq': 0,\n",
    "    'bagging_seed': 2018,\n",
    "    'verbose': 0,\n",
    "    'num_threads':16\n",
    "}\n",
    "\n",
    "trainingScores = []\n",
    "cvScores = []\n",
    "predictionsBasedOnKFolds = pd.DataFrame(data=[], index=y_train.index, \\\n",
    "                                        columns=['prediction'])\n",
    "\n",
    "for train_index, cv_index in k_fold.split(np.zeros(len(X_train)), \\\n",
    "                                          y_train.ravel()):\n",
    "    X_train_fold, X_cv_fold = X_train.iloc[train_index], \\\n",
    "        X_train.iloc[cv_index]\n",
    "    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n",
    "        y_train.iloc[cv_index]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n",
    "    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n",
    "    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=2000,\n",
    "                   valid_sets=lgb_eval, early_stopping_rounds=200)\n",
    "    \n",
    "    loglossTraining = log_loss(y_train_fold, gbm.predict(X_train_fold, \\\n",
    "                                num_iteration=gbm.best_iteration))\n",
    "    trainingScores.append(loglossTraining)\n",
    "    \n",
    "    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n",
    "        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration) \n",
    "    loglossCV = log_loss(y_cv_fold, \\\n",
    "        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n",
    "    cvScores.append(loglossCV)\n",
    "    \n",
    "    print('Training Log Loss: ', loglossTraining)\n",
    "    print('CV Log Loss: ', loglossCV)\n",
    "    \n",
    "loglossLightGBMGradientBoosting = log_loss(y_train, \\\n",
    "        predictionsBasedOnKFolds.prediction)\n",
    "print('LightGBM Gradient Boosting Log Loss: ', \\\n",
    "        loglossLightGBMGradientBoosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction-recall curve on Training Set \n",
    "preds, average_precision = plotResults(y_train, \\\n",
    "                        predictionsBasedOnKFolds.loc[:,'prediction'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction-recall curve on Test Set \n",
    "predictions = pd.Series(data=gbm.predict(X_test, \\\n",
    "                num_iteration=gbm.best_iteration), index=X_test.index)\n",
    "preds, average_precision = plotResults(y_test, predictions, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision at 75% recall\n",
    "preds, precision = precisionAnalysis(preds, \"anomalyScore\", 0.75)\n",
    "print(\"Precision at 75% recall\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection using Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample dataset first\n",
    "oversample_multiplier = 100\n",
    "\n",
    "X_train_original = X_train.copy()\n",
    "y_train_original = y_train.copy()\n",
    "X_test_original = X_test.copy()\n",
    "y_test_original = y_test.copy()\n",
    "\n",
    "X_train_oversampled = X_train.copy()\n",
    "y_train_oversampled = y_train.copy()\n",
    "X_train_oversampled = X_train_oversampled.append( \\\n",
    "        [X_train_oversampled[y_train==1]]*oversample_multiplier, \\\n",
    "        ignore_index=False)\n",
    "y_train_oversampled = y_train_oversampled.append( \\\n",
    "        [y_train_oversampled[y_train==1]]*oversample_multiplier, \\\n",
    "        ignore_index=False)\n",
    "\n",
    "X_train = X_train_oversampled.copy()\n",
    "y_train = y_train_oversampled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build autoencoder solution\n",
    "model = Sequential()\n",
    "model.add(Dense(units=40, activation='linear', \\\n",
    "                activity_regularizer=regularizers.l1(10e-5), \\\n",
    "                input_dim=29,name='hidden_layer'))\n",
    "model.add(Dropout(0.02))\n",
    "model.add(Dense(units=29, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(x=X_train, y=X_train,\n",
    "                    epochs=num_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.20,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction-recall curve on Training Set \n",
    "predictionsTrain = model.predict(X_train_original, verbose=1)\n",
    "anomalyScoresAETrain = anomalyScores(X_train_original, predictionsTrain)\n",
    "preds, average_precision = plotResults(y_train_original, \\\n",
    "                                      anomalyScoresAETrain, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction-recall curve on Test Set \n",
    "predictions = model.predict(X_test, verbose=1)\n",
    "anomalyScoresAE = anomalyScores(X_test, predictions)\n",
    "preds, average_precision = plotResults(y_test, anomalyScoresAE, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision at 75% recall\n",
    "preds, precision = precisionAnalysis(preds, \"anomalyScore\", 0.75)\n",
    "print(\"Precision at 75% recall\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraud Detection using Semi-supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unsuperivsed learning (i.e., autoencoder representations) \n",
    "# Feed into supervised model to build a semi-supervised solution\n",
    "layer_name = 'hidden_layer'\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input, \\\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output_train = intermediate_layer_model.predict(X_train_original)\n",
    "intermediate_output_test = intermediate_layer_model.predict(X_test_original)\n",
    "\n",
    "intermediate_output_trainDF = \\\n",
    "    pd.DataFrame(data=intermediate_output_train,index=X_train_original.index)\n",
    "intermediate_output_testDF = \\\n",
    "    pd.DataFrame(data=intermediate_output_test,index=X_test_original.index)\n",
    "\n",
    "X_train = X_train_original.merge(intermediate_output_trainDF, \\\n",
    "                                 left_index=True,right_index=True)\n",
    "X_test = X_test_original.merge(intermediate_output_testDF, \\\n",
    "                               left_index=True,right_index=True)\n",
    "y_train = y_train_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train supervised model\n",
    "trainingScores = []\n",
    "cvScores = []\n",
    "predictionsBasedOnKFolds = pd.DataFrame(data=[],index=y_train.index, \\\n",
    "                                        columns=['prediction'])\n",
    "\n",
    "for train_index, cv_index in k_fold.split(np.zeros(len(X_train)), \\\n",
    "                                          y_train.ravel()):\n",
    "    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n",
    "        X_train.iloc[cv_index,:]\n",
    "    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n",
    "        y_train.iloc[cv_index]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n",
    "    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n",
    "    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=5000,\n",
    "                   valid_sets=lgb_eval, early_stopping_rounds=200)\n",
    "    \n",
    "    loglossTraining = log_loss(y_train_fold, \n",
    "                                gbm.predict(X_train_fold, \\\n",
    "                                num_iteration=gbm.best_iteration))\n",
    "    trainingScores.append(loglossTraining)\n",
    "    \n",
    "    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n",
    "        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration) \n",
    "    loglossCV = log_loss(y_cv_fold, \\\n",
    "            predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n",
    "    cvScores.append(loglossCV)\n",
    "    \n",
    "    print('Training Log Loss: ', loglossTraining)\n",
    "    print('CV Log Loss: ', loglossCV)\n",
    "    \n",
    "loglossLightGBMGradientBoosting = log_loss(y_train, \\\n",
    "                        predictionsBasedOnKFolds.loc[:,'prediction'])\n",
    "print('LightGBM Gradient Boosting Log Loss: ', \\\n",
    "                        loglossLightGBMGradientBoosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction-recall curve on Training Set \n",
    "preds, average_precision = plotResults(y_train, \\\n",
    "                        predictionsBasedOnKFolds.loc[:,'prediction'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction-recall curve on Test Set \n",
    "predictions = pd.Series(data=gbm.predict(X_test, \\\n",
    "                    num_iteration=gbm.best_iteration),index=X_test.index)\n",
    "preds, average_precision = plotResults(y_test, predictions, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision at 75% recall\n",
    "preds, precision = precisionAnalysis(preds, \"anomalyScore\", 0.75)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See whether supervised features (\"V\" features) are more or less\n",
    "# important than unsupervised features\n",
    "featuresImportance = pd.DataFrame(data=list(gbm.feature_importance()), \\\n",
    "                        index=X_train.columns,columns=['featImportance'])\n",
    "featuresImportance = featuresImportance/featuresImportance.sum()\n",
    "featuresImportance.sort_values(by='featImportance', \\\n",
    "                               ascending=False,inplace=True)\n",
    "featuresImportance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Train a two-layer overcomplete autoencoder (nodes > original # input features) with a dropout of 5% and a ReLU activation function. Use 60 nodes.\n",
    "\n",
    "Feed the hidden layer representations into a supervised model, train the model on the training set, and evaluate the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use oversampled data for autoencoder training\n",
    "X_train = X_train_oversampled.copy()\n",
    "y_train = y_train_oversampled.copy()\n",
    "X_test = X_test_original.copy()\n",
    "\n",
    "# Build autoencoder solution\n",
    "model = Sequential()\n",
    "model.add(Dense(units=#Fill in\n",
    "                , activation=#Fill in\n",
    "                , activity_regularizer=regularizers.l1(10e-5), \\\n",
    "                input_dim=29,name='hidden_layer'))\n",
    "model.add(Dropout(#Fill in))\n",
    "model.add(Dense(units=29, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(x=X_train, y=X_train,\n",
    "                    epochs=num_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.20,\n",
    "                    verbose=1)\n",
    "\n",
    "# Generate unsuperivsed learning (i.e., autoencoder representations) \n",
    "# Feed into supervised model to build a semi-supervised solution\n",
    "layer_name = 'hidden_layer'\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input, \\\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output_train = intermediate_layer_model.predict(X_train_original)\n",
    "intermediate_output_test = intermediate_layer_model.predict(X_test_original)\n",
    "\n",
    "intermediate_output_trainDF = \\\n",
    "    pd.DataFrame(data=intermediate_output_train,index=X_train_original.index)\n",
    "intermediate_output_testDF = \\\n",
    "    pd.DataFrame(data=intermediate_output_test,index=X_test_original.index)\n",
    "\n",
    "X_train = X_train_original.merge(intermediate_output_trainDF, \\\n",
    "                                 left_index=True,right_index=True)\n",
    "X_test = X_test_original.merge(intermediate_output_testDF, \\\n",
    "                               left_index=True,right_index=True)\n",
    "y_train = y_train_original.copy()\n",
    "\n",
    "# Now train supervised model\n",
    "trainingScores = []\n",
    "cvScores = []\n",
    "predictionsBasedOnKFolds = pd.DataFrame(data=[],index=y_train.index, \\\n",
    "                                        columns=['prediction'])\n",
    "\n",
    "for train_index, cv_index in k_fold.split(np.zeros(len(X_train)), \\\n",
    "                                          y_train.ravel()):\n",
    "    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n",
    "        X_train.iloc[cv_index,:]\n",
    "    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n",
    "        y_train.iloc[cv_index]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n",
    "    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n",
    "    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=5000,\n",
    "                   valid_sets=lgb_eval, early_stopping_rounds=200)\n",
    "    \n",
    "    loglossTraining = log_loss(y_train_fold, \n",
    "                                gbm.predict(X_train_fold, \\\n",
    "                                num_iteration=gbm.best_iteration))\n",
    "    trainingScores.append(loglossTraining)\n",
    "    \n",
    "    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n",
    "        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration) \n",
    "    loglossCV = log_loss(y_cv_fold, \\\n",
    "            predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n",
    "    cvScores.append(loglossCV)\n",
    "    \n",
    "    print('Training Log Loss: ', loglossTraining)\n",
    "    print('CV Log Loss: ', loglossCV)\n",
    "    \n",
    "loglossLightGBMGradientBoosting = log_loss(y_train, \\\n",
    "                        predictionsBasedOnKFolds.loc[:,'prediction'])\n",
    "print('LightGBM Gradient Boosting Log Loss: ', \\\n",
    "                        loglossLightGBMGradientBoosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction-recall curve on Training Set \n",
    "preds, average_precision = plotResults(y_train, \\\n",
    "                        predictionsBasedOnKFolds.loc[:,'prediction'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction-recall curve on Test Set \n",
    "predictions = pd.Series(data=gbm.predict(X_test, \\\n",
    "                    num_iteration=gbm.best_iteration),index=X_test.index)\n",
    "preds, average_precision = plotResults(y_test, predictions, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision at 75% recall\n",
    "preds, precision = precisionAnalysis(preds, \"anomalyScore\", 0.75)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use oversampled data for autoencoder training\n",
    "X_train = X_train_oversampled.copy()\n",
    "y_train = y_train_oversampled.copy()\n",
    "X_test = X_test_original.copy()\n",
    "\n",
    "# Build autoencoder solution\n",
    "model = Sequential()\n",
    "model.add(Dense(units=60, activation='relu', \\\n",
    "                activity_regularizer=regularizers.l1(10e-5), \\\n",
    "                input_dim=29,name='hidden_layer'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(units=29, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(x=X_train, y=X_train,\n",
    "                    epochs=num_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.20,\n",
    "                    verbose=1)\n",
    "\n",
    "# Generate unsuperivsed learning (i.e., autoencoder representations) \n",
    "# Feed into supervised model to build a semi-supervised solution\n",
    "layer_name = 'hidden_layer'\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input, \\\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output_train = intermediate_layer_model.predict(X_train_original)\n",
    "intermediate_output_test = intermediate_layer_model.predict(X_test_original)\n",
    "\n",
    "intermediate_output_trainDF = \\\n",
    "    pd.DataFrame(data=intermediate_output_train,index=X_train_original.index)\n",
    "intermediate_output_testDF = \\\n",
    "    pd.DataFrame(data=intermediate_output_test,index=X_test_original.index)\n",
    "\n",
    "X_train = X_train_original.merge(intermediate_output_trainDF, \\\n",
    "                                 left_index=True,right_index=True)\n",
    "X_test = X_test_original.merge(intermediate_output_testDF, \\\n",
    "                               left_index=True,right_index=True)\n",
    "y_train = y_train_original.copy()\n",
    "\n",
    "# Now train supervised model\n",
    "trainingScores = []\n",
    "cvScores = []\n",
    "predictionsBasedOnKFolds = pd.DataFrame(data=[],index=y_train.index, \\\n",
    "                                        columns=['prediction'])\n",
    "\n",
    "for train_index, cv_index in k_fold.split(np.zeros(len(X_train)), \\\n",
    "                                          y_train.ravel()):\n",
    "    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n",
    "        X_train.iloc[cv_index,:]\n",
    "    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n",
    "        y_train.iloc[cv_index]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n",
    "    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n",
    "    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=5000,\n",
    "                   valid_sets=lgb_eval, early_stopping_rounds=200)\n",
    "    \n",
    "    loglossTraining = log_loss(y_train_fold, \n",
    "                                gbm.predict(X_train_fold, \\\n",
    "                                num_iteration=gbm.best_iteration))\n",
    "    trainingScores.append(loglossTraining)\n",
    "    \n",
    "    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n",
    "        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration) \n",
    "    loglossCV = log_loss(y_cv_fold, \\\n",
    "            predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n",
    "    cvScores.append(loglossCV)\n",
    "    \n",
    "    print('Training Log Loss: ', loglossTraining)\n",
    "    print('CV Log Loss: ', loglossCV)\n",
    "    \n",
    "loglossLightGBMGradientBoosting = log_loss(y_train, \\\n",
    "                        predictionsBasedOnKFolds.loc[:,'prediction'])\n",
    "print('LightGBM Gradient Boosting Log Loss: ', \\\n",
    "                        loglossLightGBMGradientBoosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction-recall curve on Training Set \n",
    "preds, average_precision = plotResults(y_train, \\\n",
    "                        predictionsBasedOnKFolds.loc[:,'prediction'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction-recall curve on Test Set \n",
    "predictions = pd.Series(data=gbm.predict(X_test, \\\n",
    "                    num_iteration=gbm.best_iteration),index=X_test.index)\n",
    "preds, average_precision = plotResults(y_test, predictions, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision at 75% recall\n",
    "preds, precision = precisionAnalysis(preds, \"anomalyScore\", 0.75)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we explored autoencoders using TensorFlow and Keras\n",
    "\n",
    "Next, we explored semi-supervised learning, leveraging both supervised and unsupervised learning to build a credit card fraud detection system.\n",
    "\n",
    "We simulated a partially labeled dataset by dropping 90% of the labels in the credit card transactions dataset.\n",
    "\n",
    "We then learned representations using an autocoder and fed these into a supervised model. We compared the semi-supervised solution against both a standalone supervised solution and a standalone unsupervised solution.\n",
    "\n",
    "The semi-supervised solution fared the best.\n",
    "\n",
    "Congratulations, you've finished this course! \n",
    "Go build more semi-supervised solutions in your field!\n",
    "\n",
    "The next course in the Inside Unsupervised Learning series is Generative Models and Recommender Systems.\n",
    "https://learning.oreilly.com/live-training/courses/inside-unsupervised-learning-generative-models-and-recommender-systems/0636920283515/\n",
    "\n",
    "You could also learn more about Unsupervised Learning in my book, Hands-on Unsupervised Learning Using Python.\n",
    "https://www.unsupervisedlearningbook.com/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
