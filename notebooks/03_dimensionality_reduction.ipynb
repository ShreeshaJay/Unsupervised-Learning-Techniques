{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inside unsupervised learning: Anomaly detection using dimensionality reduction\n",
    "## Build systems to detect rare events such as fraud, cyberattacks, and more\n",
    "### by Ankur A. Patel + O'Reilly Media, Inc.\n",
    "\n",
    "## Overview - Part A\n",
    "In this notebook, you will understand how to:\n",
    "#1. How to reduce the computational complexity of working with very large datasets\n",
    "#2. How to remove irrelevant information in datasets and surface the most salient information\n",
    "\n",
    "Specifically, we will use linear and nonlinear dimensionality reduction to do #1 and #2 above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Let's load in the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, pickle, gzip\n",
    "\n",
    "'''Data Viz'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn import preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "os.chdir('/home/jovyan/')\n",
    "current_path = os.getcwd()\n",
    "file = '/data/mnist_data/mnist.pkl.gz'\n",
    "\n",
    "f = gzip.open(current_path+file, 'rb')\n",
    "train_set, validation_set, test_set = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "X_train, y_train = train_set[0], train_set[1]\n",
    "X_validation, y_validation = validation_set[0], validation_set[1]\n",
    "X_test, y_test = test_set[0], test_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify shape of datasets\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of X_validation: \", X_validation.shape)\n",
    "print(\"Shape of y_validation: \", y_validation.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Pandas DataFrames from the datasets\n",
    "train_index = range(0,len(X_train))\n",
    "validation_index = range(len(X_train), \\\n",
    "                         len(X_train)+len(X_validation))\n",
    "test_index = range(len(X_train)+len(X_validation), \\\n",
    "                   len(X_train)+len(X_validation)+len(X_test))\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train,index=train_index)\n",
    "y_train = pd.Series(data=y_train,index=train_index)\n",
    "\n",
    "X_validation = pd.DataFrame(data=X_validation,index=validation_index)\n",
    "y_validation = pd.Series(data=y_validation,index=validation_index)\n",
    "\n",
    "X_test = pd.DataFrame(data=X_test,index=test_index)\n",
    "y_test = pd.Series(data=y_test,index=test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Describe the training matrix\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the labels\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_digit(example):\n",
    "    label = y_train.loc[0]\n",
    "    image = X_train.loc[example,:].values.reshape([28,28])\n",
    "    plt.title('Example: %d  Label: %d' % (example, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View the first digit\n",
    "view_digit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(series):\n",
    "    label_binarizer = pp.LabelBinarizer()\n",
    "    label_binarizer.fit(range(max(series)+1))\n",
    "    return label_binarizer.transform(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverse_one_hot(originalSeries, newSeries):\n",
    "    label_binarizer = pp.LabelBinarizer()\n",
    "    label_binarizer.fit(range(max(originalSeries)+1))\n",
    "    return label_binarizer.inverse_transform(newSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create one-hot vectors for the labels\n",
    "y_train_oneHot = one_hot(y_train)\n",
    "y_validation_oneHot = one_hot(y_validation)\n",
    "y_test_oneHot = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show one-hot vector for example 0, which is the number 5\n",
    "y_train_oneHot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Dimensionality Reduction\n",
    "Let's start with linear dimensionality reduction methods first before we turn to non-linear methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 784\n",
    "whiten = False\n",
    "random_state = 2018\n",
    "\n",
    "pca = PCA(n_components=n_components, whiten=whiten, \\\n",
    "          random_state=random_state)\n",
    "\n",
    "X_train_PCA = pca.fit_transform(X_train)\n",
    "X_train_PCA = pd.DataFrame(data=X_train_PCA, index=train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Percentage of Variance Captured by 784 principal components\n",
    "print(\"Variance Explained by all 784 principal components: \", \\\n",
    "      sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Percentage of Variance Captured by X principal components\n",
    "importanceOfPrincipalComponents = \\\n",
    "    pd.DataFrame(data=pca.explained_variance_ratio_)\n",
    "importanceOfPrincipalComponents = importanceOfPrincipalComponents.T\n",
    "\n",
    "print('Variance Captured by First 10 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:9].sum(axis=1).values)\n",
    "print('Variance Captured by First 20 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:19].sum(axis=1).values)\n",
    "print('Variance Captured by First 50 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:49].sum(axis=1).values)\n",
    "print('Variance Captured by First 100 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:99].sum(axis=1).values)\n",
    "print('Variance Captured by First 200 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:199].sum(axis=1).values)\n",
    "print('Variance Captured by First 300 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:299].sum(axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "sns.barplot(data=importanceOfPrincipalComponents.loc[:,0:9],color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scatterPlot(xDF, yDF, algoName):\n",
    "    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)\n",
    "    tempDF = pd.concat((tempDF,yDF), axis=1, join=\"inner\")\n",
    "    tempDF.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n",
    "    sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", \\\n",
    "               data=tempDF, fit_reg=False)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(\"Separation of Observations using \"+algoName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatterPlot(X_train_PCA, y_train, \"PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_scatter = pd.DataFrame(data=X_train.loc[:,[350,406]], index=X_train.index)\n",
    "X_train_scatter = pd.concat((X_train_scatter,y_train), axis=1, join=\"inner\")\n",
    "X_train_scatter.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n",
    "sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", data=X_train_scatter, fit_reg=False)\n",
    "ax = plt.gca()\n",
    "ax.set_title(\"Separation of Observations Using Original Feature Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Incremental PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_components = 784\n",
    "batch_size = None\n",
    "\n",
    "incrementalPCA = IncrementalPCA(n_components=n_components, \\\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "X_train_incrementalPCA = incrementalPCA.fit_transform(X_train)\n",
    "X_train_incrementalPCA = \\\n",
    "    pd.DataFrame(data=X_train_incrementalPCA, index=train_index)\n",
    "\n",
    "X_validation_incrementalPCA = incrementalPCA.transform(X_validation)\n",
    "X_validation_incrementalPCA = \\\n",
    "    pd.DataFrame(data=X_validation_incrementalPCA, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_incrementalPCA, y_train, \"Incremental PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sparse PCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "n_components = 100\n",
    "alpha = 0.0001\n",
    "random_state = 2018\n",
    "n_jobs = -1\n",
    "\n",
    "sparsePCA = SparsePCA(n_components=n_components, \\\n",
    "                alpha=alpha, random_state=random_state, n_jobs=n_jobs)\n",
    "\n",
    "sparsePCA.fit(X_train.loc[:10000,:])\n",
    "X_train_sparsePCA = sparsePCA.transform(X_train)\n",
    "X_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=train_index)\n",
    "\n",
    "X_validation_sparsePCA = sparsePCA.transform(X_validation)\n",
    "X_validation_sparsePCA = \\\n",
    "    pd.DataFrame(data=X_validation_sparsePCA, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_sparsePCA, y_train, \"Sparse PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Kernel PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "n_components = 100\n",
    "kernel = 'rbf'\n",
    "gamma = None\n",
    "random_state = 2018\n",
    "n_jobs = 1\n",
    "\n",
    "kernelPCA = KernelPCA(n_components=n_components, kernel=kernel, \\\n",
    "                      gamma=gamma, n_jobs=n_jobs, random_state=random_state)\n",
    "\n",
    "kernelPCA.fit(X_train.loc[:10000,:])\n",
    "X_train_kernelPCA = kernelPCA.transform(X_train)\n",
    "X_train_kernelPCA = pd.DataFrame(data=X_train_kernelPCA,index=train_index)\n",
    "\n",
    "X_validation_kernelPCA = kernelPCA.transform(X_validation)\n",
    "X_validation_kernelPCA = \\\n",
    "    pd.DataFrame(data=X_validation_kernelPCA, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_kernelPCA, y_train, \"Kernel PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Singular Value Decomposition\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_components = 200\n",
    "algorithm = 'randomized'\n",
    "n_iter = 5\n",
    "random_state = 2018\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_components, algorithm=algorithm, \\\n",
    "                   n_iter=n_iter, random_state=random_state)\n",
    "\n",
    "X_train_svd = svd.fit_transform(X_train)\n",
    "X_train_svd = pd.DataFrame(data=X_train_svd, index=train_index)\n",
    "\n",
    "X_validation_svd = svd.transform(X_validation)\n",
    "X_validation_svd = pd.DataFrame(data=X_validation_svd, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_svd, y_train, \"Singular Value Decomposition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gaussian Random Projection\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "n_components = 'auto'\n",
    "eps = 0.5\n",
    "random_state = 2018\n",
    "\n",
    "GRP = GaussianRandomProjection(n_components=n_components, eps=eps, \\\n",
    "                               random_state=random_state)\n",
    "\n",
    "X_train_GRP = GRP.fit_transform(X_train)\n",
    "X_train_GRP = pd.DataFrame(data=X_train_GRP, index=train_index)\n",
    "\n",
    "X_validation_GRP = GRP.transform(X_validation)\n",
    "X_validation_GRP = pd.DataFrame(data=X_validation_GRP, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_GRP, y_train, \"Gaussian Random Projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sparse Random Projection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "n_components = 'auto'\n",
    "density = 'auto'\n",
    "eps = 0.5\n",
    "dense_output = False\n",
    "random_state = 2018\n",
    "\n",
    "SRP = SparseRandomProjection(n_components=n_components, \\\n",
    "        density=density, eps=eps, dense_output=dense_output, \\\n",
    "        random_state=random_state)\n",
    "\n",
    "X_train_SRP = SRP.fit_transform(X_train)\n",
    "X_train_SRP = pd.DataFrame(data=X_train_SRP, index=train_index)\n",
    "\n",
    "X_validation_SRP = SRP.transform(X_validation)\n",
    "X_validation_SRP = pd.DataFrame(data=X_validation_SRP, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_SRP, y_train, \"Sparse Random Projection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Dimensionality Reduction\n",
    "Now, let's move to non-linear methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Isomap\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "n_neighbors = 5\n",
    "n_components = 10\n",
    "n_jobs = 4\n",
    "\n",
    "isomap = Isomap(n_neighbors=n_neighbors, \\\n",
    "                n_components=n_components, n_jobs=n_jobs)\n",
    "\n",
    "isomap.fit(X_train.loc[0:5000,:])\n",
    "X_train_isomap = isomap.transform(X_train)\n",
    "X_train_isomap = pd.DataFrame(data=X_train_isomap, index=train_index)\n",
    "\n",
    "X_validation_isomap = isomap.transform(X_validation)\n",
    "X_validation_isomap = pd.DataFrame(data=X_validation_isomap, \\\n",
    "                                   index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_isomap, y_train, \"Isomap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multidimensional Scaling\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "n_components = 2\n",
    "n_init = 12\n",
    "max_iter = 1200\n",
    "metric = True\n",
    "n_jobs = 4\n",
    "random_state = 2018\n",
    "\n",
    "mds = MDS(n_components=n_components, n_init=n_init, max_iter=max_iter, \\\n",
    "          metric=metric, n_jobs=n_jobs, random_state=random_state)\n",
    "\n",
    "X_train_mds = mds.fit_transform(X_train.loc[0:1000,:])\n",
    "X_train_mds = pd.DataFrame(data=X_train_mds, index=train_index[0:1001])\n",
    "\n",
    "scatterPlot(X_train_mds, y_train, \"Multidimensional Scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally Linear Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Locally Linear Embedding (LLE)\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "n_neighbors = 10\n",
    "n_components = 2\n",
    "method = 'modified'\n",
    "n_jobs = 4\n",
    "random_state = 2018\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, \\\n",
    "        n_components=n_components, method=method, \\\n",
    "        random_state=random_state, n_jobs=n_jobs)\n",
    "\n",
    "lle.fit(X_train.loc[0:5000,:])\n",
    "X_train_lle = lle.transform(X_train)\n",
    "X_train_lle = pd.DataFrame(data=X_train_lle, index=train_index)\n",
    "\n",
    "X_validation_lle = lle.transform(X_validation)\n",
    "X_validation_lle = pd.DataFrame(data=X_validation_lle, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_lle, y_train, \"Locally Linear Embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "n_components = 2\n",
    "learning_rate = 300\n",
    "perplexity = 30\n",
    "early_exaggeration = 12\n",
    "init = 'random'\n",
    "random_state = 2018\n",
    "\n",
    "tSNE = TSNE(n_components=n_components, learning_rate=learning_rate, \\\n",
    "            perplexity=perplexity, early_exaggeration=early_exaggeration, \\\n",
    "            init=init, random_state=random_state)\n",
    "\n",
    "X_train_tSNE = tSNE.fit_transform(X_train_PCA.loc[:5000,:9])\n",
    "X_train_tSNE = pd.DataFrame(data=X_train_tSNE, index=train_index[:5001])\n",
    "\n",
    "scatterPlot(X_train_tSNE, y_train, \"t-SNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Dimensionality Reduction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mini-batch dictionary learning\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "\n",
    "n_components = 50\n",
    "alpha = 1\n",
    "batch_size = 200\n",
    "n_iter = 25\n",
    "random_state = 2018\n",
    "\n",
    "miniBatchDictLearning = MiniBatchDictionaryLearning( \\\n",
    "                        n_components=n_components, alpha=alpha, \\\n",
    "                        batch_size=batch_size, n_iter=n_iter, \\\n",
    "                        random_state=random_state)\n",
    "\n",
    "miniBatchDictLearning.fit(X_train.loc[:,:10000])\n",
    "X_train_miniBatchDictLearning = miniBatchDictLearning.fit_transform(X_train)\n",
    "X_train_miniBatchDictLearning = pd.DataFrame( \\\n",
    "    data=X_train_miniBatchDictLearning, index=train_index)\n",
    "\n",
    "X_validation_miniBatchDictLearning = \\\n",
    "    miniBatchDictLearning.transform(X_validation)\n",
    "X_validation_miniBatchDictLearning = \\\n",
    "    pd.DataFrame(data=X_validation_miniBatchDictLearning, \\\n",
    "    index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_miniBatchDictLearning, y_train, \\\n",
    "            \"Mini-batch Dictionary Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Independent Component Analysis\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "n_components = 25\n",
    "algorithm = 'parallel'\n",
    "whiten = True\n",
    "max_iter = 100\n",
    "random_state = 2018\n",
    "\n",
    "fastICA = FastICA(n_components=n_components, algorithm=algorithm, \\\n",
    "                  whiten=whiten, max_iter=max_iter, random_state=random_state)\n",
    "\n",
    "X_train_fastICA = fastICA.fit_transform(X_train)\n",
    "X_train_fastICA = pd.DataFrame(data=X_train_fastICA, index=train_index)\n",
    "\n",
    "X_validation_fastICA = fastICA.transform(X_validation)\n",
    "X_validation_fastICA = pd.DataFrame(data=X_validation_fastICA, \\\n",
    "                                    index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_fastICA, y_train, \"Independent Component Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "On the test set, apply PCA with 700, 300, and 100 components and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = #Fill in\n",
    "whiten = #Fill in\n",
    "random_state = 2018\n",
    "\n",
    "pca = PCA(#Fill in)\n",
    "    \n",
    "X_test_PCA = pca.fit_transform(X_test)\n",
    "X_test_PCA = pd.DataFrame(data=X_test_PCA, index=test_index)\n",
    "scatterPlot(X_test_PCA, y_test, \"PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "On the test set, apply dictionary learning with 100 and 30 components and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mini-batch dictionary learning\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "\n",
    "n_components = #Fill in\n",
    "alpha = #Fill in\n",
    "batch_size = #Fill in\n",
    "n_iter = #Fill in\n",
    "random_state = 2018\n",
    "\n",
    "miniBatchDictLearning = MiniBatchDictionaryLearning(#Fill in)\n",
    "\n",
    "miniBatchDictLearning.fit(X_test)\n",
    "X_test_miniBatchDictLearning = miniBatchDictLearning.fit_transform(X_test)\n",
    "X_test_miniBatchDictLearning = pd.DataFrame( \\\n",
    "    data=X_test_miniBatchDictLearning, index=test_index)\n",
    "\n",
    "scatterPlot(X_test_miniBatchDictLearning, y_test, \\\n",
    "            \"Mini-batch Dictionary Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to the Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 1 Answers\n",
    "# Principal Component Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 700\n",
    "whiten = False\n",
    "random_state = 2018\n",
    "\n",
    "pca = PCA(n_components=n_components, whiten=whiten, \\\n",
    "          random_state=random_state)\n",
    "    \n",
    "X_test_PCA = pca.fit_transform(X_test)\n",
    "X_test_PCA = pd.DataFrame(data=X_test_PCA, index=test_index)\n",
    "scatterPlot(X_test_PCA, y_test, \"PCA\")\n",
    "\n",
    "n_components = 300\n",
    "whiten = False\n",
    "random_state = 2018\n",
    "\n",
    "pca = PCA(n_components=n_components, whiten=whiten, \\\n",
    "          random_state=random_state)\n",
    "    \n",
    "X_test_PCA = pca.fit_transform(X_test)\n",
    "X_test_PCA = pd.DataFrame(data=X_test_PCA, index=test_index)\n",
    "scatterPlot(X_test_PCA, y_test, \"PCA\")\n",
    "\n",
    "n_components = 100\n",
    "whiten = False\n",
    "random_state = 2018\n",
    "\n",
    "pca = PCA(n_components=n_components, whiten=whiten, \\\n",
    "          random_state=random_state)\n",
    "    \n",
    "X_test_PCA = pca.fit_transform(X_test)\n",
    "X_test_PCA = pd.DataFrame(data=X_test_PCA, index=test_index)\n",
    "scatterPlot(X_test_PCA, y_test, \"PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 2 Answers\n",
    "# Mini-batch dictionary learning\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "\n",
    "n_components = 100\n",
    "alpha = 1\n",
    "batch_size = 200\n",
    "n_iter = 25\n",
    "random_state = 2018\n",
    "\n",
    "miniBatchDictLearning = MiniBatchDictionaryLearning( \\\n",
    "                        n_components=n_components, alpha=alpha, \\\n",
    "                        batch_size=batch_size, n_iter=n_iter, \\\n",
    "                        random_state=random_state)\n",
    "\n",
    "miniBatchDictLearning.fit(X_test)\n",
    "X_test_miniBatchDictLearning = miniBatchDictLearning.fit_transform(X_test)\n",
    "X_test_miniBatchDictLearning = pd.DataFrame( \\\n",
    "    data=X_test_miniBatchDictLearning, index=test_index)\n",
    "\n",
    "scatterPlot(X_test_miniBatchDictLearning, y_test, \\\n",
    "            \"Mini-batch Dictionary Learning\")\n",
    "\n",
    "n_components = 30\n",
    "alpha = 1\n",
    "batch_size = 200\n",
    "n_iter = 25\n",
    "random_state = 2018\n",
    "\n",
    "miniBatchDictLearning = MiniBatchDictionaryLearning( \\\n",
    "                        n_components=n_components, alpha=alpha, \\\n",
    "                        batch_size=batch_size, n_iter=n_iter, \\\n",
    "                        random_state=random_state)\n",
    "\n",
    "miniBatchDictLearning.fit(X_test)\n",
    "X_test_miniBatchDictLearning = miniBatchDictLearning.fit_transform(X_test)\n",
    "X_test_miniBatchDictLearning = pd.DataFrame( \\\n",
    "    data=X_test_miniBatchDictLearning, index=test_index)\n",
    "\n",
    "scatterPlot(X_test_miniBatchDictLearning, y_test, \\\n",
    "            \"Mini-batch Dictionary Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion to Part A\n",
    "In this notebook, we explored linear and non-linear dimensionality reduction methods on the MNIST dataset.\n",
    "In the next notebook, we will apply these methods on a credit card transactions dataset to develop a fraud detection system.\n",
    "Fraud detection is one real world application of anomaly detection."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
