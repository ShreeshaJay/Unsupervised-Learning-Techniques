{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inside unsupervised learning: Anomaly detection using dimensionality reduction\n",
    "## Build systems to detect rare events such as fraud, cyberattacks, and more\n",
    "### by Ankur A. Patel + O'Reilly Media, Inc.\n",
    "\n",
    "## Overview - Part B\n",
    "In this notebook, you will understand how to:\n",
    "#1. Apply dimensionality reduction to perform anomaly detection\n",
    "\n",
    "Specifically, we will use dimensionality reduction to build a credit card fraud detection system.\n",
    "After this course, you will be able to use your knowledge of dimensionality reduction to build other types of anomaly detection systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Load in credit card transactions dataset and prepare it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time\n",
    "import pickle, gzip\n",
    "\n",
    "'''Data Viz'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "data1 = pd.read_csv('../data/credit_card_data/credit_card_data_part1.csv')\n",
    "data2 = pd.read_csv('../data/credit_card_data/credit_card_data_part2.csv')\n",
    "data = data1.append(data2)\n",
    "\n",
    "dataX = data.copy().drop(['Class'],axis=1)\n",
    "dataY = data['Class'].copy()\n",
    "\n",
    "featuresToScale = dataX.columns\n",
    "sX = pp.StandardScaler(copy=True)\n",
    "dataX.loc[:,featuresToScale] = sX.fit_transform(dataX[featuresToScale])\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(dataX, dataY, test_size=0.33, \\\n",
    "                    random_state=2018, stratify=dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def anomalyScores(originalDF, reducedDF):\n",
    "    loss = np.sum((np.array(originalDF)-np.array(reducedDF))**2, axis=1)\n",
    "    loss = pd.Series(data=loss,index=originalDF.index)\n",
    "    loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotResults(trueLabels, anomalyScores, returnPreds = False):\n",
    "    preds = pd.concat([trueLabels, anomalyScores], axis=1)\n",
    "    preds.columns = ['trueLabel', 'anomalyScore']\n",
    "    precision, recall, thresholds = \\\n",
    "        precision_recall_curve(preds['trueLabel'],preds['anomalyScore'])\n",
    "    average_precision = \\\n",
    "        average_precision_score(preds['trueLabel'],preds['anomalyScore'])\n",
    "    \n",
    "    plt.step(recall, precision, color='k', alpha=0.7, where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    \n",
    "    plt.title('Precision-Recall curve: Average Precision = \\\n",
    "    {0:0.2f}'.format(average_precision))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(preds['trueLabel'], \\\n",
    "                                     preds['anomalyScore'])\n",
    "    areaUnderROC = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\n",
    "    plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic: \\\n",
    "    Area under the curve = {0:0.2f}'.format(areaUnderROC))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    if returnPreds==True:\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scatterPlot(xDF, yDF, algoName):\n",
    "    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)\n",
    "    tempDF = pd.concat((tempDF,yDF), axis=1, join=\"inner\")\n",
    "    tempDF.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n",
    "    sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", \\\n",
    "               data=tempDF, fit_reg=False)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(\"Separation of Observations using \"+algoName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design PCA-based anomaly detection system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "# 30 principal components\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 30\n",
    "whiten = False\n",
    "random_state = 2018\n",
    "\n",
    "pca = PCA(n_components=n_components, whiten=whiten, \\\n",
    "          random_state=random_state)\n",
    "\n",
    "X_train_PCA = pca.fit_transform(X_train)\n",
    "X_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)\n",
    "\n",
    "X_train_PCA_inverse = pca.inverse_transform(X_train_PCA)\n",
    "X_train_PCA_inverse = pd.DataFrame(data=X_train_PCA_inverse, \\\n",
    "                                   index=X_train.index)\n",
    "\n",
    "scatterPlot(X_train_PCA, y_train, \"PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anomalyScoresPCA = anomalyScores(X_train, X_train_PCA_inverse)\n",
    "preds = plotResults(y_train, anomalyScoresPCA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "# 27 principal components\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 27\n",
    "whiten = False\n",
    "random_state = 2018\n",
    "\n",
    "pca = PCA(n_components=n_components, whiten=whiten, \\\n",
    "          random_state=random_state)\n",
    "\n",
    "X_train_PCA = pca.fit_transform(X_train)\n",
    "X_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)\n",
    "\n",
    "X_train_PCA_inverse = pca.inverse_transform(X_train_PCA)\n",
    "X_train_PCA_inverse = pd.DataFrame(data=X_train_PCA_inverse, \\\n",
    "                                   index=X_train.index)\n",
    "\n",
    "scatterPlot(X_train_PCA, y_train, \"PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalyScoresPCA = anomalyScores(X_train, X_train_PCA_inverse)\n",
    "preds = plotResults(y_train, anomalyScoresPCA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.sort_values(by=\"anomalyScore\",ascending=False,inplace=True)\n",
    "cutoff = 350\n",
    "predsTop = preds[:cutoff]\n",
    "print(\"Precision: \",np.round(predsTop. \\\n",
    "            anomalyScore[predsTop.trueLabel==1].count()/cutoff,2))\n",
    "print(\"Recall: \",np.round(predsTop. \\\n",
    "            anomalyScore[predsTop.trueLabel==1].count()/y_train.sum(),2))\n",
    "print(\"Fraud Caught out of 330 Cases:\", predsTop.trueLabel.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse PCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "n_components = 27\n",
    "alpha = 0.0001\n",
    "random_state = 2018\n",
    "n_jobs = -1\n",
    "\n",
    "sparsePCA = SparsePCA(n_components=n_components, \\\n",
    "                alpha=alpha, random_state=random_state, n_jobs=n_jobs)\n",
    "\n",
    "sparsePCA.fit(X_train.loc[:,:])\n",
    "X_train_sparsePCA = sparsePCA.transform(X_train)\n",
    "X_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=X_train.index)\n",
    "\n",
    "scatterPlot(X_train_sparsePCA, y_train, \"Sparse PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparsePCA_inverse = np.array(X_train_sparsePCA). \\\n",
    "    dot(sparsePCA.components_) + np.array(X_train.mean(axis=0))\n",
    "X_train_sparsePCA_inverse = \\\n",
    "    pd.DataFrame(data=X_train_sparsePCA_inverse, index=X_train.index)\n",
    "\n",
    "anomalyScoresSparsePCA = anomalyScores(X_train, X_train_sparsePCA_inverse)\n",
    "preds = plotResults(y_train, anomalyScoresSparsePCA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "n_components = 27\n",
    "kernel = 'rbf'\n",
    "gamma = None\n",
    "fit_inverse_transform = True\n",
    "random_state = 2018\n",
    "n_jobs = 1\n",
    "\n",
    "kernelPCA = KernelPCA(n_components=n_components, kernel=kernel, \\\n",
    "                gamma=gamma, fit_inverse_transform= \\\n",
    "                fit_inverse_transform, n_jobs=n_jobs, \\\n",
    "                random_state=random_state)\n",
    "\n",
    "kernelPCA.fit(X_train.iloc[:2000])\n",
    "X_train_kernelPCA = kernelPCA.transform(X_train)\n",
    "X_train_kernelPCA = pd.DataFrame(data=X_train_kernelPCA, \\\n",
    "                                 index=X_train.index)\n",
    "\n",
    "X_train_kernelPCA_inverse = kernelPCA.inverse_transform(X_train_kernelPCA)\n",
    "X_train_kernelPCA_inverse = pd.DataFrame(data=X_train_kernelPCA_inverse, \\\n",
    "                                         index=X_train.index)\n",
    "\n",
    "scatterPlot(X_train_kernelPCA, y_train, \"Kernel PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalyScoresKernelPCA = anomalyScores(X_train, X_train_kernelPCA_inverse)\n",
    "preds = plotResults(y_train, anomalyScoresKernelPCA, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Random Projection-based anomaly detection system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Random Projection\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "n_components = 27\n",
    "eps = None\n",
    "random_state = 2018\n",
    "\n",
    "GRP = GaussianRandomProjection(n_components=n_components, \\\n",
    "                               eps=eps, random_state=random_state)\n",
    "\n",
    "X_train_GRP = GRP.fit_transform(X_train)\n",
    "X_train_GRP = pd.DataFrame(data=X_train_GRP, index=X_train.index)\n",
    "\n",
    "scatterPlot(X_train_GRP, y_train, \"Gaussian Random Projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_GRP_inverse = np.array(X_train_GRP).dot(GRP.components_)\n",
    "X_train_GRP_inverse = pd.DataFrame(data=X_train_GRP_inverse, \\\n",
    "                                   index=X_train.index)\n",
    "\n",
    "anomalyScoresGRP = anomalyScores(X_train, X_train_GRP_inverse)\n",
    "preds = plotResults(y_train, anomalyScoresGRP, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse Random Projection\n",
    "\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "n_components = 27\n",
    "density = 'auto'\n",
    "eps = .01\n",
    "dense_output = True\n",
    "random_state = 2018\n",
    "\n",
    "SRP = SparseRandomProjection(n_components=n_components, \\\n",
    "        density=density, eps=eps, dense_output=dense_output, \\\n",
    "                                random_state=random_state)\n",
    "\n",
    "X_train_SRP = SRP.fit_transform(X_train)\n",
    "X_train_SRP = pd.DataFrame(data=X_train_SRP, index=X_train.index)\n",
    "\n",
    "scatterPlot(X_train_SRP, y_train, \"Sparse Random Projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_SRP_inverse = np.array(X_train_SRP).dot(SRP.components_.todense())\n",
    "X_train_SRP_inverse = pd.DataFrame(data=X_train_SRP_inverse, index=X_train.index)\n",
    "\n",
    "anomalyScoresSRP = anomalyScores(X_train, X_train_SRP_inverse)\n",
    "plotResults(y_train, anomalyScoresSRP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Dictionary Learning-based anomaly detection system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Mini-batch dictionary learning\n",
    "\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "\n",
    "n_components = 28\n",
    "alpha = 1\n",
    "batch_size = 200\n",
    "n_iter = 10\n",
    "random_state = 2018\n",
    "\n",
    "miniBatchDictLearning = MiniBatchDictionaryLearning( \\\n",
    "    n_components=n_components, alpha=alpha, batch_size=batch_size, \\\n",
    "    n_iter=n_iter, random_state=random_state)\n",
    "\n",
    "miniBatchDictLearning.fit(X_train)\n",
    "X_train_miniBatchDictLearning = \\\n",
    "    miniBatchDictLearning.fit_transform(X_train)\n",
    "X_train_miniBatchDictLearning = \\\n",
    "    pd.DataFrame(data=X_train_miniBatchDictLearning, index=X_train.index)\n",
    "\n",
    "scatterPlot(X_train_miniBatchDictLearning, y_train, \\\n",
    "            \"Mini-batch Dictionary Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_miniBatchDictLearning_inverse = \\\n",
    "    np.array(X_train_miniBatchDictLearning). \\\n",
    "    dot(miniBatchDictLearning.components_)\n",
    "\n",
    "X_train_miniBatchDictLearning_inverse = \\\n",
    "    pd.DataFrame(data=X_train_miniBatchDictLearning_inverse, \\\n",
    "                 index=X_train.index)\n",
    "\n",
    "anomalyScoresMiniBatchDictLearning = anomalyScores(X_train, \\\n",
    "    X_train_miniBatchDictLearning_inverse)\n",
    "preds = plotResults(y_train, anomalyScoresMiniBatchDictLearning, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design ICA-based anomaly detection system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent Component Analysis\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "n_components = 27\n",
    "algorithm = 'parallel'\n",
    "whiten = True\n",
    "max_iter = 200\n",
    "random_state = 2018\n",
    "\n",
    "fastICA = FastICA(n_components=n_components, \\\n",
    "    algorithm=algorithm, whiten=whiten, max_iter=max_iter, \\\n",
    "    random_state=random_state)\n",
    "\n",
    "X_train_fastICA = fastICA.fit_transform(X_train)\n",
    "X_train_fastICA = pd.DataFrame(data=X_train_fastICA, index=X_train.index)\n",
    "\n",
    "X_train_fastICA_inverse = fastICA.inverse_transform(X_train_fastICA)\n",
    "X_train_fastICA_inverse = pd.DataFrame(data=X_train_fastICA_inverse, \\\n",
    "                                       index=X_train.index)\n",
    "\n",
    "scatterPlot(X_train_fastICA, y_train, \"Independent Component Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalyScoresFastICA = anomalyScores(X_train, X_train_fastICA_inverse)\n",
    "plotResults(y_train, anomalyScoresFastICA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Anomaly Detection Systems on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on Test Set\n",
    "\n",
    "X_test_PCA = pca.transform(X_test)\n",
    "X_test_PCA = pd.DataFrame(data=X_test_PCA, index=X_test.index)\n",
    "\n",
    "X_test_PCA_inverse = pca.inverse_transform(X_test_PCA)\n",
    "X_test_PCA_inverse = pd.DataFrame(data=X_test_PCA_inverse, \\\n",
    "                                  index=X_test.index)\n",
    "\n",
    "scatterPlot(X_test_PCA, y_test, \"PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalyScoresPCA = anomalyScores(X_test, X_test_PCA_inverse)\n",
    "preds = plotResults(y_test, anomalyScoresPCA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent Component Analysis on Test Set\n",
    "\n",
    "X_test_fastICA = fastICA.transform(X_test)\n",
    "X_test_fastICA = pd.DataFrame(data=X_test_fastICA, index=X_test.index)\n",
    "\n",
    "X_test_fastICA_inverse = fastICA.inverse_transform(X_test_fastICA)\n",
    "X_test_fastICA_inverse = pd.DataFrame(data=X_test_fastICA_inverse, \\\n",
    "                                      index=X_test.index)\n",
    "\n",
    "scatterPlot(X_test_fastICA, y_test, \"Independent Component Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalyScoresFastICA = anomalyScores(X_test, X_test_fastICA_inverse)\n",
    "plotResults(y_test, anomalyScoresFastICA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary Learning on Test Set\n",
    "\n",
    "X_test_miniBatchDictLearning = miniBatchDictLearning.transform(X_test)\n",
    "X_test_miniBatchDictLearning = \\\n",
    "    pd.DataFrame(data=X_test_miniBatchDictLearning, index=X_test.index)\n",
    "\n",
    "scatterPlot(X_test_miniBatchDictLearning, y_test, \\\n",
    "            \"Mini-batch Dictionary Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_miniBatchDictLearning_inverse = \\\n",
    "    np.array(X_test_miniBatchDictLearning). \\\n",
    "    dot(miniBatchDictLearning.components_)\n",
    "\n",
    "X_test_miniBatchDictLearning_inverse = \\\n",
    "    pd.DataFrame(data=X_test_miniBatchDictLearning_inverse, \\\n",
    "                 index=X_test.index)\n",
    "\n",
    "anomalyScoresMiniBatchDictLearning = anomalyScores(X_test, \\\n",
    "    X_test_miniBatchDictLearning_inverse)\n",
    "preds = plotResults(y_test, anomalyScoresMiniBatchDictLearning, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Fit Sparse PCA on the training set with 25 components and an alpha of 0.0001 and apply the Sparse PCA anomaly detection solution on the test set and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sparse PCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "n_components = #Fill in\n",
    "alpha = #Fill in\n",
    "random_state = 2018\n",
    "n_jobs = -1\n",
    "\n",
    "sparsePCA = SparsePCA(#Fill in)\n",
    "\n",
    "# Fit Sparse PCA model on Train Set\n",
    "\n",
    "sparsePCA.fit(#Fill in)\n",
    "\n",
    "# Apply Sparse PCA model on Test Set\n",
    "\n",
    "X_test_sparsePCA = #Fill in\n",
    "X_test_sparsePCA = pd.DataFrame(data=X_test_sparsePCA, index=X_test.index)\n",
    "\n",
    "X_test_sparsePCA_inverse = np.array(X_test_sparsePCA). \\\n",
    "    dot(sparsePCA.components_) + np.array(X_test.mean(axis=0))\n",
    "X_test_sparsePCA_inverse = \\\n",
    "    pd.DataFrame(data=X_test_sparsePCA_inverse, index=X_test.index)\n",
    "\n",
    "anomalyScoresSparsePCA = anomalyScores(X_test, X_test_sparsePCA_inverse)\n",
    "preds = plotResults(y_test, anomalyScoresSparsePCA, True)\n",
    "\n",
    "scatterPlot(X_test_sparsePCA, y_test, \"Sparse PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to the Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sparse PCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "n_components = 25\n",
    "alpha = 0.0001\n",
    "random_state = 2018\n",
    "n_jobs = -1\n",
    "\n",
    "sparsePCA = SparsePCA(n_components=n_components, \\\n",
    "                alpha=alpha, random_state=random_state, n_jobs=n_jobs)\n",
    "\n",
    "# Fit Sparse PCA model on Train Set\n",
    "\n",
    "sparsePCA.fit(X_train.loc[:,:])\n",
    "\n",
    "# Apply Sparse PCA model on Test Set\n",
    "\n",
    "X_test_sparsePCA = sparsePCA.transform(X_test)\n",
    "X_test_sparsePCA = pd.DataFrame(data=X_test_sparsePCA, index=X_test.index)\n",
    "\n",
    "X_test_sparsePCA_inverse = np.array(X_test_sparsePCA). \\\n",
    "    dot(sparsePCA.components_) + np.array(X_test.mean(axis=0))\n",
    "X_test_sparsePCA_inverse = \\\n",
    "    pd.DataFrame(data=X_test_sparsePCA_inverse, index=X_test.index)\n",
    "\n",
    "anomalyScoresSparsePCA = anomalyScores(X_test, X_test_sparsePCA_inverse)\n",
    "preds = plotResults(y_test, anomalyScoresSparsePCA, True)\n",
    "\n",
    "scatterPlot(X_test_sparsePCA, y_test, \"Sparse PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion to Part B\n",
    "In this notebook, we applied PCA, random projection, dictionary learning, and independent component analysis dimensionality reduction methods on a credit card transactions dataset and developed a fraud detection system. This is one real world application of anomaly detection. This solution worked reasonably well on the test set.\n",
    "\n",
    "Congratulations, you've finished this course! \n",
    "Go build more anomaly detection systems in your field!\n",
    "\n",
    "The next course in the Inside Unsupervised Learning series is Group Segmentation using Clustering.\n",
    "https://www.oreilly.com/live-training/courses/inside-unsupervised-learning-group-segmentation-using-clustering/0636920283478/\n",
    "\n",
    "You could also learn more about Unsupervised Learning in my book, Hands-on Unsupervised Learning Using Python.\n",
    "https://www.unsupervisedlearningbook.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
