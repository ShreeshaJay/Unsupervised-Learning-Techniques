{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inside unsupervised learning: Group segmentation using clustering\n",
    "## Build systems to segment users into distinct and homogenous groups\n",
    "### by Ankur A. Patel + O'Reilly Media, Inc.\n",
    "\n",
    "## Overview - Part B\n",
    "In this notebook, you will understand how to:\n",
    "#1 Perform good feature engineering\n",
    "#2 Cluster users into distinct and homogenous groups\n",
    "#3 Efficiently label a dataset after clustering, turning an unsupervised problem into a semi-supervised one\n",
    "\n",
    "Specifically, we will cluster borrowers from Lending Club into distinct groups using the clustering algorithms we introduced in Part A of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Let's load in the Lending Club dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, re, pickle, gzip\n",
    "\n",
    "'''Data Viz'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "'''Algorithms'''\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import fastcluster\n",
    "from scipy.cluster.hierarchy import dendrogram, cophenet, fcluster\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "os.chdir('/home/jovyan/')\n",
    "current_path = os.getcwd()\n",
    "file = '/data/lending_club_data/LoanStats3a.csv'\n",
    "data = pd.read_csv(current_path + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to keep\n",
    "columnsToKeep = ['loan_amnt','funded_amnt','funded_amnt_inv','term', \\\n",
    "                 'int_rate','installment','grade','sub_grade', \\\n",
    "                 'emp_length','home_ownership','annual_inc', \\\n",
    "                 'verification_status','pymnt_plan','purpose', \\\n",
    "                 'addr_state','dti','delinq_2yrs','earliest_cr_line', \\\n",
    "                 'mths_since_last_delinq','mths_since_last_record', \\\n",
    "                 'open_acc','pub_rec','revol_bal','revol_util', \\\n",
    "                 'total_acc','initial_list_status','out_prncp', \\\n",
    "                 'out_prncp_inv','total_pymnt','total_pymnt_inv', \\\n",
    "                 'total_rec_prncp','total_rec_int','total_rec_late_fee', \\\n",
    "                 'recoveries','collection_recovery_fee','last_pymnt_d', \\\n",
    "                 'last_pymnt_amnt']\n",
    "\n",
    "data = data.loc[:,columnsToKeep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore shape of data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first 5 rows of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform features from string to numeric\n",
    "for i in [\"term\",\"int_rate\",\"emp_length\",\"revol_util\"]:\n",
    "    data.loc[:,i] = \\\n",
    "        data.loc[:,i].apply(lambda x: re.sub(\"[^0-9]\", \"\", str(x)))\n",
    "    data.loc[:,i] = pd.to_numeric(data.loc[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which features are numerical\n",
    "numericalFeats = [x for x in data.columns if data[x].dtype != 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display NaNs by feature\n",
    "nanCounter = np.isnan(data.loc[:,numericalFeats]).sum()\n",
    "nanCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute NaNs with mean \n",
    "fillWithMean = ['loan_amnt','funded_amnt','funded_amnt_inv','term', \\\n",
    "                'int_rate','installment','emp_length','annual_inc',\\\n",
    "                'dti','open_acc','revol_bal','revol_util','total_acc',\\\n",
    "                'out_prncp','out_prncp_inv','total_pymnt', \\\n",
    "                'total_pymnt_inv','total_rec_prncp','total_rec_int', \\\n",
    "                'last_pymnt_amnt']\n",
    "\n",
    "# Impute NaNs with zero\n",
    "fillWithZero = ['delinq_2yrs','mths_since_last_delinq', \\\n",
    "                'mths_since_last_record','pub_rec','total_rec_late_fee', \\\n",
    "                'recoveries','collection_recovery_fee']\n",
    "\n",
    "# Perform imputation\n",
    "im = pp.Imputer(strategy='mean')   \n",
    "data.loc[:,fillWithMean] = im.fit_transform(data[fillWithMean])\n",
    "\n",
    "data.loc[:,fillWithZero] = data.loc[:,fillWithZero].fillna(value=0,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs one last time\n",
    "nanCounter = np.isnan(data.loc[:,numericalFeats]).sum()\n",
    "nanCounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "data['installmentOverLoanAmnt'] = data.installment/data.loan_amnt\n",
    "data['loanAmntOverIncome'] = data.loan_amnt/data.annual_inc\n",
    "data['revol_balOverIncome'] = data.revol_bal/data.annual_inc\n",
    "data['totalPymntOverIncome'] = data.total_pymnt/data.annual_inc\n",
    "data['totalPymntInvOverIncome'] = data.total_pymnt_inv/data.annual_inc\n",
    "data['totalRecPrncpOverIncome'] = data.total_rec_prncp/data.annual_inc\n",
    "data['totalRecIncOverIncome'] = data.total_rec_int/data.annual_inc\n",
    "\n",
    "newFeats = ['installmentOverLoanAmnt','loanAmntOverIncome', \\\n",
    "            'revol_balOverIncome','totalPymntOverIncome', \\\n",
    "           'totalPymntInvOverIncome','totalRecPrncpOverIncome', \\\n",
    "            'totalRecIncOverIncome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training\n",
    "numericalPlusNewFeats = numericalFeats+newFeats\n",
    "X_train = data.loc[:,numericalPlusNewFeats]\n",
    "\n",
    "# Scale data\n",
    "sX = pp.StandardScaler()\n",
    "X_train.loc[:,:] = sX.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View new columns\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate labels for evaluation\n",
    "labels = data.grade\n",
    "labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing labels\n",
    "labels = labels.fillna(value=\"Z\")\n",
    "\n",
    "# Convert labels to numerical values\n",
    "lbl = pp.LabelEncoder()\n",
    "lbl.fit(list(labels.values))\n",
    "labels = pd.Series(data=lbl.transform(labels.values), name=\"grade\")\n",
    "\n",
    "# Store as y_train\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View new labels vs. original labels\n",
    "labelsOriginalVSNew = pd.concat([labels, data.grade],axis=1)\n",
    "labelsOriginalVSNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare loan grades with interest rates\n",
    "interestAndGrade = pd.DataFrame(data=[data.int_rate,labels])\n",
    "interestAndGrade = interestAndGrade.T\n",
    "\n",
    "interestAndGrade.groupby(\"grade\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to evaluate goodness of the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeCluster(clusterDF, labelsDF):\n",
    "    countByCluster = \\\n",
    "        pd.DataFrame(data=clusterDF['cluster'].value_counts())\n",
    "    countByCluster.reset_index(inplace=True,drop=False)\n",
    "    countByCluster.columns = ['cluster','clusterCount']\n",
    "        \n",
    "    preds = pd.concat([labelsDF,clusterDF], axis=1)\n",
    "    preds.columns = ['trueLabel','cluster']\n",
    "    \n",
    "    countByLabel = pd.DataFrame(data=preds.groupby('trueLabel').count())\n",
    "        \n",
    "    countMostFreq = pd.DataFrame(data=preds.groupby('cluster').agg( \\\n",
    "        lambda x:x.value_counts().iloc[0]))\n",
    "    countMostFreq.reset_index(inplace=True,drop=False)\n",
    "    countMostFreq.columns = ['cluster','countMostFrequent']\n",
    "    \n",
    "    accuracyDF = countMostFreq.merge(countByCluster, \\\n",
    "        left_on=\"cluster\",right_on=\"cluster\")\n",
    "    \n",
    "    overallAccuracy = accuracyDF.countMostFrequent.sum()/ \\\n",
    "        accuracyDF.clusterCount.sum()\n",
    "    \n",
    "    accuracyByLabel = accuracyDF.countMostFrequent/ \\\n",
    "        accuracyDF.clusterCount\n",
    "    \n",
    "    return countByCluster, countByLabel, countMostFreq, \\\n",
    "        accuracyDF, overallAccuracy, accuracyByLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Application #1 - K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 10\n",
    "n_init = 10\n",
    "max_iter = 300\n",
    "tol = 0.0001\n",
    "random_state = 2018\n",
    "n_jobs = 2\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=n_init, \\\n",
    "                max_iter=max_iter, tol=tol, \\\n",
    "                random_state=random_state, n_jobs=n_jobs)\n",
    "\n",
    "kMeans_inertia = pd.DataFrame(data=[],index=range(10,31), \\\n",
    "                              columns=['inertia'])\n",
    "\n",
    "overallAccuracy_kMeansDF = pd.DataFrame(data=[], \\\n",
    "    index=range(10,31),columns=['overallAccuracy'])\n",
    "\n",
    "for n_clusters in range(10,31):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=n_init, \\\n",
    "                    max_iter=max_iter, tol=tol, \\\n",
    "                    random_state=random_state, n_jobs=n_jobs)\n",
    "\n",
    "    kmeans.fit(X_train)\n",
    "    kMeans_inertia.loc[n_clusters] = kmeans.inertia_\n",
    "    X_train_kmeansClustered = kmeans.predict(X_train)\n",
    "    X_train_kmeansClustered = pd.DataFrame(data= \\\n",
    "        X_train_kmeansClustered, index=X_train.index, \\\n",
    "        columns=['cluster'])\n",
    "    \n",
    "    countByCluster_kMeans, countByLabel_kMeans, \\\n",
    "    countMostFreq_kMeans, accuracyDF_kMeans, \\\n",
    "    overallAccuracy_kMeans, accuracyByLabel_kMeans = \\\n",
    "    analyzeCluster(X_train_kmeansClustered, y_train)\n",
    "    \n",
    "    overallAccuracy_kMeansDF.loc[n_clusters] = \\\n",
    "        overallAccuracy_kMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy as the number of clusters increases\n",
    "overallAccuracy_kMeansDF.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy by cluster\n",
    "accuracyByLabel_kMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Application #2 - Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcluster\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "Z = fastcluster.linkage_vector(X_train, method='ward', \\\n",
    "                               metric='euclidean')\n",
    "\n",
    "Z_dataFrame = pd.DataFrame(data=Z,columns=['clusterOne', \\\n",
    "                'clusterTwo','distance','newClusterSize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first 20 clustered rows\n",
    "Z_dataFrame[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View last 20 clustered rows\n",
    "Z_dataFrame[42521:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut off tree and see how clusters are left\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "distance_threshold = 100\n",
    "clusters = fcluster(Z, distance_threshold, criterion='distance')\n",
    "X_train_hierClustered = pd.DataFrame(data=clusters, \\\n",
    "    index=X_train.index,columns=['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters left after cutting off the tree\n",
    "print(\"Number of distinct clusters: \", \\\n",
    "      len(X_train_hierClustered['cluster'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalute overall accuracy from hierarchical clustering\n",
    "countByCluster_hierClust, countByLabel_hierClust, \\\n",
    "    countMostFreq_hierClust, accuracyDF_hierClust, \\\n",
    "    overallAccuracy_hierClust, accuracyByLabel_hierClust = \\\n",
    "    analyzeCluster(X_train_hierClustered, y_train)\n",
    "\n",
    "print(\"Overall accuracy from hierarchical clustering: \", \\\n",
    "      overallAccuracy_hierClust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View accuracy by cluster\n",
    "print(\"Accuracy by cluster for hierarchical clustering\")\n",
    "accuracyByLabel_hierClust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Application #3 - HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "min_cluster_size = 20\n",
    "min_samples = 20\n",
    "alpha = 1.0\n",
    "cluster_selection_method = 'leaf'\n",
    "\n",
    "hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, \\\n",
    "    min_samples=min_samples, alpha=alpha, \\\n",
    "    cluster_selection_method=cluster_selection_method)\n",
    "\n",
    "X_train_hdbscanClustered = hdb.fit_predict(X_train)\n",
    "X_train_hdbscanClustered = pd.DataFrame(data= \\\n",
    "    X_train_hdbscanClustered, index=X_train.index, \\\n",
    "    columns=['cluster'])\n",
    "\n",
    "countByCluster_hdbscan, countByLabel_hdbscan, \\\n",
    "    countMostFreq_hdbscan, accuracyDF_hdbscan, \\\n",
    "    overallAccuracy_hdbscan, accuracyByLabel_hdbscan = \\\n",
    "    analyzeCluster(X_train_hdbscanClustered, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View overall accuracy from HDBSCAN\n",
    "print(\"Overall accuracy from HDBSCAN: \", overallAccuracy_hdbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View count of entities within each cluster\n",
    "print(\"Cluster results for HDBSCAN\")\n",
    "countByCluster_hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View accuracy by cluster\n",
    "accuracyByLabel_hdbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Adjust parameters for K-means, hierarchical clusters, and HDBSCAN per the instructions and calculate overall accuracy again on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "Use 50 clusters and recalculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = #Fill in\n",
    "n_init = 10\n",
    "max_iter = 300\n",
    "tol = 0.0001\n",
    "random_state = 2018\n",
    "n_jobs = 2\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=n_init, \\\n",
    "                max_iter=max_iter, tol=tol, \\\n",
    "                random_state=random_state, n_jobs=n_jobs)\n",
    "\n",
    "kmeans.fit(#Fill in)\n",
    "X_train_kmeansClustered = kmeans.predict(#Fill in)\n",
    "X_train_kmeansClustered = pd.DataFrame(data= \\\n",
    "        X_train_kmeansClustered, index=X_train.index, \\\n",
    "        columns=['cluster'])\n",
    "    \n",
    "countByCluster_kMeans, countByLabel_kMeans, \\\n",
    "countMostFreq_kMeans, accuracyDF_kMeans, \\\n",
    "overallAccuracy_kMeans, accuracyByLabel_kMeans = \\\n",
    "analyzeCluster(X_train_kmeansClustered, y_train)\n",
    "\n",
    "print(\"Overall accuracy from k-means: \", \\\n",
    "      overallAccuracy_kMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering\n",
    "Use distance threshold (i.e., tree cutoff of 50) and recalculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcluster\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "Z = fastcluster.linkage_vector(X_train, method='ward', \\\n",
    "                               metric='euclidean')\n",
    "\n",
    "Z_dataFrame = pd.DataFrame(data=Z,columns=['clusterOne', \\\n",
    "                'clusterTwo','distance','newClusterSize'])\n",
    "\n",
    "# Cut off tree and see how clusters are left\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "distance_threshold = #Fill in\n",
    "clusters = fcluster(Z, distance_threshold, criterion='distance')\n",
    "X_train_hierClustered = pd.DataFrame(data=clusters, \\\n",
    "    index=X_train.index,columns=['cluster'])\n",
    "\n",
    "# Number of clusters left after cutting off the tree\n",
    "print(\"Number of distinct clusters: \", \\\n",
    "      len(X_train_hierClustered['cluster'].unique()))\n",
    "\n",
    "# Evalute overall accuracy from hierarchical clustering\n",
    "countByCluster_hierClust, countByLabel_hierClust, \\\n",
    "    countMostFreq_hierClust, accuracyDF_hierClust, \\\n",
    "    overallAccuracy_hierClust, accuracyByLabel_hierClust = \\\n",
    "    analyzeCluster(X_train_hierClustered, y_train)\n",
    "\n",
    "print(\"Overall accuracy from hierarchical clustering: \", \\\n",
    "      overallAccuracy_hierClust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN\n",
    "Use min_cluster_size of 10 and min_samples of 5 and recalculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "min_cluster_size = #Fill in\n",
    "min_samples = #Fill in\n",
    "alpha = 1.0\n",
    "cluster_selection_method = 'leaf'\n",
    "\n",
    "hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, \\\n",
    "    min_samples=min_samples, alpha=alpha, \\\n",
    "    cluster_selection_method=cluster_selection_method)\n",
    "\n",
    "X_train_hdbscanClustered = hdb.fit_predict(X_train)\n",
    "X_train_hdbscanClustered = pd.DataFrame(data= \\\n",
    "    X_train_hdbscanClustered, index=X_train.index, \\\n",
    "    columns=['cluster'])\n",
    "\n",
    "countByCluster_hdbscan, countByLabel_hdbscan, \\\n",
    "    countMostFreq_hdbscan, accuracyDF_hdbscan, \\\n",
    "    overallAccuracy_hdbscan, accuracyByLabel_hdbscan = \\\n",
    "    analyzeCluster(X_train_hdbscanClustered, y_train)\n",
    "\n",
    "print(\"Overall accuracy from HDBSCAN: \", \\\n",
    "      overallAccuracy_hdbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to the Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "# K-means\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 50\n",
    "n_init = 10\n",
    "max_iter = 300\n",
    "tol = 0.0001\n",
    "random_state = 2018\n",
    "n_jobs = 2\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=n_init, \\\n",
    "                max_iter=max_iter, tol=tol, \\\n",
    "                random_state=random_state, n_jobs=n_jobs)\n",
    "\n",
    "kmeans.fit(X_train)\n",
    "X_train_kmeansClustered = kmeans.predict(X_train)\n",
    "X_train_kmeansClustered = pd.DataFrame(data= \\\n",
    "        X_train_kmeansClustered, index=X_train.index, \\\n",
    "        columns=['cluster'])\n",
    "    \n",
    "countByCluster_kMeans, countByLabel_kMeans, \\\n",
    "countMostFreq_kMeans, accuracyDF_kMeans, \\\n",
    "overallAccuracy_kMeans, accuracyByLabel_kMeans = \\\n",
    "analyzeCluster(X_train_kmeansClustered, y_train)\n",
    "\n",
    "print(\"Overall accuracy from k-means: \", \\\n",
    "      overallAccuracy_kMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Answers\n",
    "# Hierarchical clustering\n",
    "import fastcluster\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "Z = fastcluster.linkage_vector(X_train, method='ward', \\\n",
    "                               metric='euclidean')\n",
    "\n",
    "Z_dataFrame = pd.DataFrame(data=Z,columns=['clusterOne', \\\n",
    "                'clusterTwo','distance','newClusterSize'])\n",
    "\n",
    "# Cut off tree and see how clusters are left\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "distance_threshold = 50\n",
    "clusters = fcluster(Z, distance_threshold, criterion='distance')\n",
    "X_train_hierClustered = pd.DataFrame(data=clusters, \\\n",
    "    index=X_train.index,columns=['cluster'])\n",
    "\n",
    "# Number of clusters left after cutting off the tree\n",
    "print(\"Number of distinct clusters: \", \\\n",
    "      len(X_train_hierClustered['cluster'].unique()))\n",
    "\n",
    "# Evalute overall accuracy from hierarchical clustering\n",
    "countByCluster_hierClust, countByLabel_hierClust, \\\n",
    "    countMostFreq_hierClust, accuracyDF_hierClust, \\\n",
    "    overallAccuracy_hierClust, accuracyByLabel_hierClust = \\\n",
    "    analyzeCluster(X_train_hierClustered, y_train)\n",
    "\n",
    "print(\"Overall accuracy from hierarchical clustering: \", \\\n",
    "      overallAccuracy_hierClust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 Answers\n",
    "# HDBSCAN\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "min_cluster_size = 10\n",
    "min_samples = 5\n",
    "alpha = 1.0\n",
    "cluster_selection_method = 'leaf'\n",
    "\n",
    "hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, \\\n",
    "    min_samples=min_samples, alpha=alpha, \\\n",
    "    cluster_selection_method=cluster_selection_method)\n",
    "\n",
    "X_train_hdbscanClustered = hdb.fit_predict(X_train)\n",
    "X_train_hdbscanClustered = pd.DataFrame(data= \\\n",
    "    X_train_hdbscanClustered, index=X_train.index, \\\n",
    "    columns=['cluster'])\n",
    "\n",
    "countByCluster_hdbscan, countByLabel_hdbscan, \\\n",
    "    countMostFreq_hdbscan, accuracyDF_hdbscan, \\\n",
    "    overallAccuracy_hdbscan, accuracyByLabel_hdbscan = \\\n",
    "    analyzeCluster(X_train_hdbscanClustered, y_train)\n",
    "\n",
    "print(\"Overall accuracy from HDBSCAN: \", \\\n",
    "      overallAccuracy_hdbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion to Part B\n",
    "In this notebook, we applied K-means, hierarchical clustering, and HDBSCAN on the dataset of Lending Club applications to group similiar applicants together.\n",
    "\n",
    "We used the labels of loan grades to see how well the clustering was able to segment the borrowers into distinct and homogenous groups based on creditworthiness.\n",
    "\n",
    "The results were OK but could be improved with better feature engineering and selection and more hyperparameter tuning.\n",
    "\n",
    "Group segementation is one real world application of clustering, and now you could use clustering methods to group users of your choice in your own field.\n",
    "\n",
    "Congratulations, you've finished this course! \n",
    "Go build more clustering systems!\n",
    "\n",
    "The next course in the Inside Unsupervised Learning series is Feature Extraction using Autoencoders and Semi-Supervised Learning.\n",
    "https://learning.oreilly.com/live-training/courses/inside-unsupervised-learning-feature-extraction-using-autoencoders-and-semi-supervised-learning/0636920283492/\n",
    "\n",
    "You could also learn more about Unsupervised Learning in my book, Hands-on Unsupervised Learning Using Python.\n",
    "https://www.unsupervisedlearningbook.com/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
